{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "DeepLearningTP2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e-WSDtOP1b7",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning - Diplomatura en ciencias de datos\n",
        "## UNC - FaMAF\n",
        "## Práctico n°1 \n",
        "\n",
        "__Integrantes:__\n",
        "* Nindirí Armenta\n",
        "* Leonardo Latini\n",
        "* Juliana Benítez\n",
        "* Florencia Alonso\n",
        "\n",
        "__Consigna:__ Se trabajará con el conjuto de datos de petfinder. La tarea es predecir la velocidad de adopción de un conjunto de mascotas. Para ello, también se dispone de [esta competencia de Kaggle](https://www.kaggle.com/t/8842af91604944a9974bd6d5a3e097c5). Se trata de una tarea de __clasificación.__\n",
        "\n",
        "_Más información del dataset:_\n",
        "\n",
        "File descriptions\n",
        "\n",
        "* train.csv - Tabular/text data for the training set\n",
        "* test.csv - Tabular/text data for the test set\n",
        "* sample_submission.csv - A sample submission file in the correct format\n",
        "* breed_labels.csv - Contains Type, and BreedName for each BreedID. Type 1 is dog, 2 is cat.\n",
        "* color_labels.csv - Contains ColorName for each ColorID\n",
        "* state_labels.csv - Contains StateName for each StateID\n",
        "\n",
        "Data Fields\n",
        "\n",
        "* PetID - Unique hash ID of pet profile\n",
        "* AdoptionSpeed - Categorical speed of adoption. \n",
        "* Type - Type of animal (1 = Dog, 2 = Cat)\n",
        "* Name - Name of pet (Empty if not named)\n",
        "* Age - Age of pet when listed, in months\n",
        "* Breed1 - Primary breed of pet (Refer to BreedLabels dictionary)\n",
        "* Breed2 - Secondary breed of pet, if pet is of mixed breed (Refer to BreedLabels dictionary)\n",
        "* Gender - Gender of pet (1 = Male, 2 = Female, 3 = Mixed, if profile represents group of pets)\n",
        "* Color1 - Color 1 of pet (Refer to ColorLabels dictionary)\n",
        "* Color2 - Color 2 of pet (Refer to ColorLabels dictionary)\n",
        "* Color3 - Color 3 of pet (Refer to ColorLabels dictionary)\n",
        "* MaturitySize - Size at maturity (1 = Small, 2 = Medium, 3 = Large, 4 = Extra Large, 0 = Not Specified)\n",
        "* FurLength - Fur length (1 = Short, 2 = Medium, 3 = Long, 0 = Not Specified)\n",
        "* Vaccinated - Pet has been vaccinated (1 = Yes, 2 = No, 3 = Not Sure)\n",
        "* Dewormed - Pet has been dewormed (1 = Yes, 2 = No, 3 = Not Sure)\n",
        "* Sterilized - Pet has been spayed / neutered (1 = Yes, 2 = No, 3 = Not Sure)\n",
        "* Health - Health Condition (1 = Healthy, 2 = Minor Injury, 3 = Serious Injury, 0 = Not Specified)\n",
        "* Quantity - Number of pets represented in profile\n",
        "* Fee - Adoption fee (0 = Free)\n",
        "* State - State location in Malaysia (Refer to StateLabels dictionary)\n",
        "* Description - Profile write-up for this pet. The primary language used is English, with some in Malay or Chinese.\n",
        "\n",
        "AdoptionSpeed\n",
        "\n",
        "* 0 - Pet was adopted on the same day as it was listed.\n",
        "* 1 - Pet was adopted between 1 and 7 days (1st week) after being listed.\n",
        "* 2 - Pet was adopted between 8 and 30 days (1st month) after being listed.\n",
        "* 3 - Pet was adopted between 31 and 90 days (2nd & 3rd month) after being listed.\n",
        "* 4 - No adoption after 100 days of being listed. (There are no pets in this dataset that waited between 90 and 100 days).\n",
        "\n",
        "\n",
        "### Práctico 2 - Redes en escalera avanzadas\n",
        "\n",
        "Este práctico es similar al práctico 1, pero agregará un paso extra que es el uso de redes en escalera avanzadas, ya sean Redes Convolucionales o Redes Recurrentes.\n",
        "\n",
        "En este caso se usará la descripción como un feature extra y todo el procesamiento que ello requiere.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "941PuzzkBlBz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "f451a000-1b9e-4c94-df01-8018dcd81df4"
      },
      "source": [
        "!pip install mlflow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.25.3)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.1.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.17.4)\n",
            "Requirement already satisfied: sqlparse in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.3.0)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.3.11)\n",
            "Requirement already satisfied: databricks-cli>=0.8.7 in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.9.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: querystring-parser in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.2.4)\n",
            "Requirement already satisfied: gitpython>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (3.0.5)\n",
            "Requirement already satisfied: docker>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (4.1.0)\n",
            "Requirement already satisfied: gorilla in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.3.0)\n",
            "Requirement already satisfied: gunicorn; platform_system != \"Windows\" in /usr/local/lib/python3.6/dist-packages (from mlflow) (20.0.0)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.6/dist-packages (from mlflow) (3.17.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (7.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from mlflow) (2.6.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from mlflow) (3.13)\n",
            "Requirement already satisfied: requests>=2.17.3 in /usr/local/lib/python3.6/dist-packages (from mlflow) (2.21.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.12.0)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.3.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->mlflow) (2018.9)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask->mlflow) (0.16.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask->mlflow) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask->mlflow) (2.10.3)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from databricks-cli>=0.8.7->mlflow) (0.8.5)\n",
            "Requirement already satisfied: configparser>=0.3.5 in /usr/local/lib/python3.6/dist-packages (from databricks-cli>=0.8.7->mlflow) (4.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.0->mlflow) (41.6.0)\n",
            "Requirement already satisfied: gitdb2>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from gitpython>=2.1.0->mlflow) (2.0.6)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from docker>=4.0.0->mlflow) (0.56.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (3.0.4)\n",
            "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.6/dist-packages (from alembic->mlflow) (1.0.4)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.6/dist-packages (from alembic->mlflow) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask->mlflow) (1.1.1)\n",
            "Requirement already satisfied: smmap2>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from gitdb2>=2.0.0->gitpython>=2.1.0->mlflow) (2.0.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF9ZLc1uavW8",
        "colab_type": "code",
        "outputId": "0a5f5947-0148-461e-ccd7-dfd0e78ae9d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "RXGZLGoNP1cT",
        "colab_type": "code",
        "outputId": "6c86bda5-b914-434d-f7f4-81329104030e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from IPython.display import SVG\n",
        "from gensim import corpora\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from pprint import pprint\n",
        "\n",
        "nltk.download([\"punkt\", \"stopwords\"]);"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur7CcMiMoykM",
        "colab_type": "code",
        "outputId": "a5808f8e-6a49-4b19-e60f-6c08ca6dd164",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s22UL6fiP1c2",
        "colab_type": "text"
      },
      "source": [
        "## Carga de los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrLf3L0wP1c6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIRECTORY = '/content/drive/My Drive/DeepLearning/petfinder data'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvwZBqMhP1dM",
        "colab_type": "code",
        "outputId": "1b36ef7a-dae7-46ee-acd8-19573ab17e91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "source": [
        "dataset = pd.read_csv(os.path.join(DATA_DIRECTORY, 'train.csv'))\n",
        "\n",
        "target_col = 'AdoptionSpeed'\n",
        "nlabels = dataset[target_col].unique().shape[0]\n",
        "\n",
        "dataset.head(3)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Type</th>\n",
              "      <th>Age</th>\n",
              "      <th>Breed1</th>\n",
              "      <th>Breed2</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Color1</th>\n",
              "      <th>Color2</th>\n",
              "      <th>Color3</th>\n",
              "      <th>MaturitySize</th>\n",
              "      <th>FurLength</th>\n",
              "      <th>Vaccinated</th>\n",
              "      <th>Dewormed</th>\n",
              "      <th>Sterilized</th>\n",
              "      <th>Health</th>\n",
              "      <th>Quantity</th>\n",
              "      <th>Fee</th>\n",
              "      <th>State</th>\n",
              "      <th>Description</th>\n",
              "      <th>AdoptionSpeed</th>\n",
              "      <th>PID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>299</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>100</td>\n",
              "      <td>41326</td>\n",
              "      <td>Nibble is a 3+ month old ball of cuteness. He ...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>307</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>150</td>\n",
              "      <td>41401</td>\n",
              "      <td>Good guard dog, very alert, active, obedience ...</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>307</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>41326</td>\n",
              "      <td>This handsome yet cute boy is up for adoption....</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Type  Age  ...  AdoptionSpeed  PID\n",
              "0     2    3  ...              2    0\n",
              "1     1    4  ...              2    3\n",
              "2     1    1  ...              2    4\n",
              "\n",
              "[3 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue8WLRBzP1da",
        "colab_type": "text"
      },
      "source": [
        "## Preproceso del texto para agregarlo como feature (manejo de secuencias)\n",
        "\n",
        "A diferencia del práctico anterior, en este caso es necesario utilizar el texto como feature extra. Pueden luego agregarlo a una red recurrente o convolucional y concatenar su salida a los atributos \"escalares\" (como \"raza\" o \"género\").\n",
        "\n",
        "\n",
        "### Tokenización"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90j-3hSaP1di",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SW = set(stopwords.words(\"english\"))\n",
        "\n",
        "def tokenize_description(description):\n",
        "    return [w.lower() for w in word_tokenize(description, language=\"english\") if w.lower() not in SW]\n",
        "\n",
        "# Fill the null values with the empty string to avoid errors with NLTK tokenization\n",
        "dataset[\"TokenizedDescription\"] = dataset[\"Description\"].fillna(value=\"\").apply(tokenize_description)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iUz4qcZnkJv",
        "colab_type": "code",
        "outputId": "dc1574db-9287-4e2a-f1ab-7c284a3539f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "dataset.head(2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Type</th>\n",
              "      <th>Age</th>\n",
              "      <th>Breed1</th>\n",
              "      <th>Breed2</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Color1</th>\n",
              "      <th>Color2</th>\n",
              "      <th>Color3</th>\n",
              "      <th>MaturitySize</th>\n",
              "      <th>FurLength</th>\n",
              "      <th>Vaccinated</th>\n",
              "      <th>Dewormed</th>\n",
              "      <th>Sterilized</th>\n",
              "      <th>Health</th>\n",
              "      <th>Quantity</th>\n",
              "      <th>Fee</th>\n",
              "      <th>State</th>\n",
              "      <th>Description</th>\n",
              "      <th>AdoptionSpeed</th>\n",
              "      <th>PID</th>\n",
              "      <th>TokenizedDescription</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>299</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>100</td>\n",
              "      <td>41326</td>\n",
              "      <td>Nibble is a 3+ month old ball of cuteness. He ...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>[nibble, 3+, month, old, ball, cuteness, ., en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>307</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>150</td>\n",
              "      <td>41401</td>\n",
              "      <td>Good guard dog, very alert, active, obedience ...</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>[good, guard, dog, ,, alert, ,, active, ,, obe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Type  Age  ...  PID                               TokenizedDescription\n",
              "0     2    3  ...    0  [nibble, 3+, month, old, ball, cuteness, ., en...\n",
              "1     1    4  ...    3  [good, guard, dog, ,, alert, ,, active, ,, obe...\n",
              "\n",
              "[2 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i37uuXiCP1dv",
        "colab_type": "text"
      },
      "source": [
        "#### Tamaño de las descripciones\n",
        "\n",
        "Un punto importante a tener en cuenta es que las descripciones tienen tamaño variable, y esto no es compatible con los algoritmos de aprendizaje automático. Por lo que hay que llevar las secuencias a un tamaño uniforme.\n",
        "\n",
        "Para definir dicho tamaño uniforme, es útil mirar qué tamaños mínimos, máximos y medios manejan las descripciones y a partir de esto establecer el tamaño máximo de la secuencia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40wUYRbbP1dz",
        "colab_type": "code",
        "outputId": "dec90fc2-c3a0-4c11-99d5-1cd0d87e756c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "pprint(dataset[\"TokenizedDescription\"].apply(len).describe())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count    10582.000000\n",
            "mean        44.418541\n",
            "std         48.464623\n",
            "min          0.000000\n",
            "25%         16.000000\n",
            "50%         31.000000\n",
            "75%         55.000000\n",
            "max        803.000000\n",
            "Name: TokenizedDescription, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zl-JwcenP1eA",
        "colab_type": "text"
      },
      "source": [
        "Vemos que más del 75% de las secuencias tienen 55 palabras o menos. Esto es un buen punto de partida, así que podemos establecer el tamaño máximo de las secuencia en 55 palabras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdHVctKLP1eE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LEN = 55"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJM9aONgP1eV",
        "colab_type": "text"
      },
      "source": [
        "## Vocabulario"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrIcHpI7P1ea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocabulary = corpora.Dictionary(dataset[\"TokenizedDescription\"])\n",
        "vocabulary.filter_extremes(no_below=1, no_above=1.0, keep_n=10000)\n",
        "# me quedo con las 10000 palabras más frecuentes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xn_jzHwP1eu",
        "colab_type": "text"
      },
      "source": [
        "## Word Embeddings (GloVe)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9KohftuctmK",
        "colab_type": "code",
        "outputId": "8ae1f22e-515a-4a6c-8c9a-b79cdc680c9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "%%bash\n",
        "mkdir -p dataset\n",
        "# Descargar los word embeddings (GloVe)\n",
        "curl -L -o ./dataset/glove.6B.zip https://cs.famaf.unc.edu.ar/~ccardellino/resources/diplodatos/glove.6B.zip\n",
        "unzip ./dataset/glove.6B.zip glove.6B.100d.txt -d ./dataset"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ./dataset/glove.6B.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0  822M    0  112k    0     0  64035      0  3:44:24  0:00:01  3:44:23 64000\r  0  822M    0 3568k    0     0  1331k      0  0:10:32  0:00:02  0:10:30 1331k\r  2  822M    2 19.2M    0     0  5436k      0  0:02:34  0:00:03  0:02:31 5436k\r  4  822M    4 35.7M    0     0  7920k      0  0:01:46  0:00:04  0:01:42 7918k\r  6  822M    6 52.0M    0     0  9475k      0  0:01:28  0:00:05  0:01:23 10.5M\r  8  822M    8 68.3M    0     0  10.3M      0  0:01:19  0:00:06  0:01:13 14.1M\r 10  822M   10 84.3M    0     0  11.0M      0  0:01:14  0:00:07  0:01:07 16.3M\r 12  822M   12  100M    0     0  11.6M      0  0:01:10  0:00:08  0:01:02 16.2M\r 14  822M   14  116M    0     0  12.1M      0  0:01:07  0:00:09  0:00:58 16.1M\r 16  822M   16  132M    0     0  12.5M      0  0:01:05  0:00:10  0:00:55 16.1M\r 18  822M   18  149M    0     0  12.8M      0  0:01:04  0:00:11  0:00:53 16.1M\r 20  822M   20  164M    0     0  13.0M      0  0:01:03  0:00:12  0:00:51 16.0M\r 22  822M   22  180M    0     0  13.2M      0  0:01:01  0:00:13  0:00:48 16.1M\r 23  822M   23  196M    0     0  13.4M      0  0:01:01  0:00:14  0:00:47 15.9M\r 25  822M   25  212M    0     0  13.5M      0  0:01:00  0:00:15  0:00:45 15.8M\r 27  822M   27  228M    0     0  13.7M      0  0:00:59  0:00:16  0:00:43 15.8M\r 29  822M   29  244M    0     0  13.8M      0  0:00:59  0:00:17  0:00:42 15.8M\r 31  822M   31  260M    0     0  13.9M      0  0:00:58  0:00:18  0:00:40 15.8M\r 33  822M   33  275M    0     0  14.0M      0  0:00:58  0:00:19  0:00:39 15.9M\r 35  822M   35  291M    0     0  14.1M      0  0:00:58  0:00:20  0:00:38 15.8M\r 37  822M   37  307M    0     0  14.2M      0  0:00:57  0:00:21  0:00:36 15.8M\r 39  822M   39  323M    0     0  14.2M      0  0:00:57  0:00:22  0:00:35 15.9M\r 41  822M   41  339M    0     0  14.3M      0  0:00:57  0:00:23  0:00:34 15.8M\r 43  822M   43  354M    0     0  14.4M      0  0:00:57  0:00:24  0:00:33 15.7M\r 45  822M   45  370M    0     0  14.4M      0  0:00:56  0:00:25  0:00:31 15.7M\r 46  822M   46  385M    0     0  14.4M      0  0:00:56  0:00:26  0:00:30 15.6M\r 48  822M   48  402M    0     0  14.5M      0  0:00:56  0:00:27  0:00:29 15.7M\r 50  822M   50  417M    0     0  14.5M      0  0:00:56  0:00:28  0:00:28 15.5M\r 52  822M   52  433M    0     0  14.6M      0  0:00:56  0:00:29  0:00:27 15.7M\r 54  822M   54  448M    0     0  14.6M      0  0:00:56  0:00:30  0:00:26 15.6M\r 56  822M   56  464M    0     0  14.7M      0  0:00:55  0:00:31  0:00:24 15.7M\r 58  822M   58  480M    0     0  14.7M      0  0:00:55  0:00:32  0:00:23 15.6M\r 60  822M   60  496M    0     0  14.7M      0  0:00:55  0:00:33  0:00:22 15.7M\r 62  822M   62  511M    0     0  14.7M      0  0:00:55  0:00:34  0:00:21 15.6M\r 64  822M   64  527M    0     0  14.8M      0  0:00:55  0:00:35  0:00:20 15.7M\r 66  822M   66  543M    0     0  14.8M      0  0:00:55  0:00:36  0:00:19 15.6M\r 67  822M   67  558M    0     0  14.8M      0  0:00:55  0:00:37  0:00:18 15.5M\r 69  822M   69  574M    0     0  14.8M      0  0:00:55  0:00:38  0:00:17 15.6M\r 71  822M   71  589M    0     0  14.8M      0  0:00:55  0:00:39  0:00:16 15.5M\r 73  822M   73  605M    0     0  14.9M      0  0:00:55  0:00:40  0:00:15 15.6M\r 75  822M   75  620M    0     0  14.9M      0  0:00:55  0:00:41  0:00:14 15.5M\r 77  822M   77  637M    0     0  14.9M      0  0:00:55  0:00:42  0:00:13 15.7M\r 79  822M   79  652M    0     0  14.9M      0  0:00:54  0:00:43  0:00:11 15.5M\r 81  822M   81  667M    0     0  14.9M      0  0:00:54  0:00:44  0:00:10 15.6M\r 83  822M   83  683M    0     0  14.9M      0  0:00:54  0:00:45  0:00:09 15.5M\r 84  822M   84  698M    0     0  14.9M      0  0:00:54  0:00:46  0:00:08 15.5M\r 86  822M   86  714M    0     0  15.0M      0  0:00:54  0:00:47  0:00:07 15.4M\r 88  822M   88  730M    0     0  15.0M      0  0:00:54  0:00:48  0:00:06 15.5M\r 90  822M   90  745M    0     0  15.0M      0  0:00:54  0:00:49  0:00:05 15.4M\r 92  822M   92  761M    0     0  15.0M      0  0:00:54  0:00:50  0:00:04 15.5M\r 94  822M   94  776M    0     0  15.0M      0  0:00:54  0:00:51  0:00:03 15.6M\r 96  822M   96  792M    0     0  15.0M      0  0:00:54  0:00:52  0:00:02 15.6M\r 98  822M   98  807M    0     0  15.0M      0  0:00:54  0:00:53  0:00:01 15.6M\r100  822M  100  822M    0     0  15.0M      0  0:00:54  0:00:54 --:--:-- 15.6M\n",
            "replace ./dataset/glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n",
            "(EOF or read error, treating as \"[N]one\" ...)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pcqMYDiP1ey",
        "colab_type": "code",
        "outputId": "38eed01d-5c24-4f3d-be5f-7231f4c53903",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embeddings_index = {}\n",
        "\n",
        "with open(\"./dataset/glove.6B.100d.txt\", \"r\") as fh:\n",
        "    for line in fh:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        if word in vocabulary.token2id:  # Only use the embeddings of words in our vocabulary\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found {} word vectors.\".format(len(embeddings_index)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 7897 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0dXNYYeP1fJ",
        "colab_type": "text"
      },
      "source": [
        "## Creación de los datasets\n",
        "\n",
        "Similar al práctico anterior, tendremos datos que serán \"one-hot-encoded\", otros serán \"embeddings\" y otros serán numéricos.\n",
        "\n",
        "El caso particular del texto es que será tratado como una secuencia de embeddings, y dichos embeddings no serán entrenados en conjunto con la red, sino que serán tomados de un modelo \"pre-entrenado\". En este caso utilizamos GloVe, pero podríamos haber utilizado otro modelo (e.g. FastText)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6EHg6u5P1fO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# It's important to always use the same one-hot length\n",
        "one_hot_columns = {\n",
        "    one_hot_col: dataset[one_hot_col].max()\n",
        "    for one_hot_col in ['Type', 'Gender', 'Color1', 'Vaccinated', 'MaturitySize', 'Sterilized', 'Health', 'FurLength']\n",
        "}\n",
        "embedded_columns = {\n",
        "    embedded_col: dataset[embedded_col].max() + 1\n",
        "    for embedded_col in ['Breed1']\n",
        "}\n",
        "numeric_columns = ['Age', 'Fee']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwxVLVCKP1fZ",
        "colab_type": "text"
      },
      "source": [
        "## Generador del conjunto de datos\n",
        "\n",
        "Dada la naturaleza de los datos de texto, y que estos representan una secuencia de datos (que se da luego a una red recurrente o convolucional), en este caso no crearemos los datasets de antemano, sino que los generaremos a medida que el algoritmo de entrenamiento los pida. \n",
        "\n",
        "En particular, es porque las secuencias de texto pueden no tener el mismo tamaño (las oraciones tienen diferente cantidad de palabras), pero para que los modelos de redes las acepten, necesitamos rellenarlas (*padding*) de manera que todas tengan el mismo tamaño.\n",
        "\n",
        "En este paso también vamos a truncar aquellas secuencias de descripciones con más de `MAX_SEQUENCE_LEN` palabras, de manera que al hacer uso de `padded_batch` no lance un error al encontrarse con secuencias de tamaño mayor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGiK-SR9hFFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "norm = scaler.fit_transform(dataset[numeric_columns])\n",
        "dfnorm = pd.DataFrame({'Age_norm':norm[:,0] ,'Fee_norm':norm[:,1]})\n",
        "dataset = pd.concat([dfnorm, dataset], axis=1)\n",
        "numeric_columns = ['Age_norm', 'Fee_norm']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H241SkB0P1fc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "71bade7a-34d8-4004-aebe-d4607768f02d"
      },
      "source": [
        "def dataset_generator(ds, test_data=False):\n",
        "    for _, row in ds.iterrows():\n",
        "        instance = {}\n",
        "        \n",
        "        # One hot encoded features\n",
        "        instance[\"direct_features\"] = np.hstack([\n",
        "            tf.keras.utils.to_categorical(row[one_hot_col] - 1, max_value)\n",
        "            for one_hot_col, max_value in one_hot_columns.items()\n",
        "        ])\n",
        "\n",
        "        # Numeric features (should be normalized beforehand)\n",
        "        for num_col in numeric_columns:\n",
        "            instance[num_col] = [row[num_col]]\n",
        "        \n",
        "        # Embedded features\n",
        "        for embedded_col in embedded_columns:\n",
        "            instance[embedded_col] = [row[embedded_col]]\n",
        "        \n",
        "        # Document to indices for text data, truncated at MAX_SEQUENCE_LEN words\n",
        "        instance[\"description\"] = vocabulary.doc2idx(\n",
        "            row[\"TokenizedDescription\"],\n",
        "            unknown_word_index=len(vocabulary)\n",
        "        )[:MAX_SEQUENCE_LEN]\n",
        "        \n",
        "        # One hot encoded target for categorical crossentropy\n",
        "        if not test_data:\n",
        "            target = tf.keras.utils.to_categorical(row[target_col], nlabels)\n",
        "            yield instance, target\n",
        "        else:\n",
        "            yield instance\n",
        "\n",
        "# Set output types of the generator (for numeric types check the type is valid)\n",
        "instance_types = {\n",
        "    \"direct_features\": tf.float32,\n",
        "    \"description\": tf.int32\n",
        "}\n",
        "\n",
        "for num_col in numeric_columns:\n",
        "    instance_types[num_col] = tf.float32\n",
        "\n",
        "for embedded_col in embedded_columns:\n",
        "    instance_types[embedded_col] = tf.int32\n",
        "        \n",
        "tf_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: dataset_generator(dataset),\n",
        "    output_types=(instance_types, tf.int32)\n",
        ")\n",
        "\n",
        "for data, target in tf_dataset.take(2):\n",
        "    pprint(data)\n",
        "    pprint(target)\n",
        "    print()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Age_norm': <tf.Tensor: id=41, shape=(1,), dtype=float32, numpy=array([-0.40931514], dtype=float32)>,\n",
            " 'Breed1': <tf.Tensor: id=42, shape=(1,), dtype=int32, numpy=array([299], dtype=int32)>,\n",
            " 'Fee_norm': <tf.Tensor: id=43, shape=(1,), dtype=float32, numpy=array([1.0101603], dtype=float32)>,\n",
            " 'description': <tf.Tensor: id=44, shape=(42,), dtype=int32, numpy=\n",
            "array([23,  2, 20, 24,  4, 10,  1, 11, 26,  1, 27,  9,  6, 21,  3,  8, 15,\n",
            "       22, 33,  7, 13, 30,  1, 29, 18, 17,  1, 12, 31, 14,  5,  6, 16,  1,\n",
            "       19, 28, 25, 32, 23,  0,  5,  1], dtype=int32)>,\n",
            " 'direct_features': <tf.Tensor: id=45, shape=(28,), dtype=float32, numpy=\n",
            "array([0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
            "       0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.], dtype=float32)>}\n",
            "<tf.Tensor: id=46, shape=(5,), dtype=int32, numpy=array([0, 0, 1, 0, 0], dtype=int32)>\n",
            "\n",
            "{'Age_norm': <tf.Tensor: id=47, shape=(1,), dtype=float32, numpy=array([-0.3548879], dtype=float32)>,\n",
            " 'Breed1': <tf.Tensor: id=48, shape=(1,), dtype=int32, numpy=array([307], dtype=int32)>,\n",
            " 'Fee_norm': <tf.Tensor: id=49, shape=(1,), dtype=float32, numpy=array([1.6479679], dtype=float32)>,\n",
            " 'description': <tf.Tensor: id=50, shape=(24,), dtype=int32, numpy=\n",
            "array([41, 42, 40, 35, 37, 35, 36, 35, 45, 50, 41, 44, 35, 46, 38, 48, 39,\n",
            "       47, 15, 43, 35, 49, 34, 34], dtype=int32)>,\n",
            " 'direct_features': <tf.Tensor: id=51, shape=(28,), dtype=float32, numpy=\n",
            "array([1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
            "       0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.], dtype=float32)>}\n",
            "<tf.Tensor: id=52, shape=(5,), dtype=int32, numpy=array([0, 0, 1, 0, 0], dtype=int32)>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEsuJs4wP1f4",
        "colab_type": "text"
      },
      "source": [
        "## Datos de entrenamiento y validación\n",
        "\n",
        "Ya generado el conjunto de datos base, tenemos que dividirlo en entrenamiento y validación. Además, como vamos a utilizar algunos datos que forman secuencias, los lotes (*batches*) de datos deben estar \"rellenados\" (*padded_batch*). \n",
        "\n",
        "Si bien rellenaremos \"todos\" los atributos, en la práctica el único que efectivamente se rellenará es el de *description* pues es el único con tamaños distintos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkQuH0j0P1f7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_SIZE = int(dataset.shape[0] * 0.8)\n",
        "DEV_SIZE = dataset.shape[0] - TRAIN_SIZE\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "shuffled_dataset = tf_dataset.shuffle(TRAIN_SIZE + DEV_SIZE, seed=42)\n",
        "# d.shuffle(shuffle_buffer_size)\n",
        "\n",
        "# Pad the datasets to the max value for all the \"non sequence\" features\n",
        "padding_shapes = (\n",
        "    {k: [-1] for k in [\"direct_features\"] + numeric_columns + list(embedded_columns.keys())},\n",
        "    [-1]\n",
        ")\n",
        "\n",
        "# Pad to MAX_SEQUENCE_LEN for sequence features\n",
        "padding_shapes[0][\"description\"] = [MAX_SEQUENCE_LEN]\n",
        "\n",
        "# Pad values are irrelevant for non padded data\n",
        "padding_values = (\n",
        "    {k: 0 for k in list(embedded_columns.keys())},\n",
        "    0\n",
        ")\n",
        "\n",
        "# Padding value for direct features should be a float\n",
        "padding_values[0][\"direct_features\"] = np.float32(0)\n",
        "\n",
        "# Padding value for numeric features\n",
        "for num_col in numeric_columns:\n",
        "    padding_values[0][num_col] = np.float32(0)\n",
        "\n",
        "# Padding value for sequential features is the vocabulary length + 1\n",
        "padding_values[0][\"description\"] = len(vocabulary) + 1\n",
        "\n",
        "train_dataset = shuffled_dataset.skip(DEV_SIZE)\\\n",
        "    .padded_batch(BATCH_SIZE, padded_shapes=padding_shapes, padding_values=padding_values)\n",
        "\n",
        "dev_dataset = shuffled_dataset.take(DEV_SIZE)\\\n",
        "    .padded_batch(BATCH_SIZE, padded_shapes=padding_shapes, padding_values=padding_values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVlWkfXBP1gR",
        "colab_type": "text"
      },
      "source": [
        "## Construyendo el modelo\n",
        "\n",
        "Al modelo anterior tenemos que agregarle la capa que maneje los embeddings de las palabras.\n",
        "\n",
        "### Matriz de embeddings de palabras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay44Q5zwP1gU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDINGS_DIM = 100  # Given by the model (in this case glove.6B.100d)\n",
        "\n",
        "embedding_matrix = np.zeros((len(vocabulary) + 2, 100))\n",
        "\n",
        "for widx, word in vocabulary.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[widx] = embedding_vector\n",
        "    else:\n",
        "        # Random normal initialization for words without embeddings\n",
        "        embedding_matrix[widx] = np.random.normal(size=(100,))  \n",
        "\n",
        "# Random normal initialization for unknown words\n",
        "embedding_matrix[len(vocabulary)] = np.random.normal(size=(100,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWkoDWiGP1gm",
        "colab_type": "text"
      },
      "source": [
        "### Definiendo los inputs del modelo\n",
        "\n",
        "Definamos los inputs del modelo, con el agregado de la capa de embeddings de palabras inicializada en `embedding_matrix`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXDIs4NoP1gp",
        "colab_type": "code",
        "outputId": "cd19f00e-0c59-4a3e-efd9-2ac5f34d9682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "from keras import regularizers\n",
        "\n",
        "# Add one input and one embedding for each embedded column\n",
        "embedding_layers = []\n",
        "inputs = []\n",
        "for embedded_col, max_value in embedded_columns.items():\n",
        "    input_layer = tf.keras.layers.Input(shape=(1,), name=embedded_col)\n",
        "    inputs.append(input_layer)\n",
        "    # Define the embedding layer\n",
        "    embedding_size = int(max_value / 4)\n",
        "    embedding_layers.append(\n",
        "        tf.squeeze(\n",
        "            tf.keras.layers.Embedding(\n",
        "                input_dim=max_value, \n",
        "                output_dim=embedding_size\n",
        "            )(input_layer), \n",
        "            axis=-2\n",
        "        )\n",
        "    )\n",
        "    print('Adding embedding of size {} for layer {}'.format(embedding_size, embedded_col))\n",
        "\n",
        "# Add the direct features already calculated\n",
        "direct_features_input = tf.keras.layers.Input(\n",
        "    shape=(sum(one_hot_columns.values()),), \n",
        "    name='direct_features'\n",
        ")\n",
        "inputs.append(direct_features_input)\n",
        "\n",
        "# Numeric cols numeric_features\n",
        "numeric_imputs = []\n",
        "for num_col in numeric_columns:\n",
        "    input_layer_num = tf.keras.layers.Input(shape=(1,), name=num_col)\n",
        "    numeric_imputs.append(input_layer_num)\n",
        "    inputs.append(input_layer_num)\n",
        "\n",
        "# Word embedding layer para la descripción\n",
        "description_input = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LEN,), name=\"description\")\n",
        "inputs.append(description_input)\n",
        "\n",
        "word_embeddings_layer = tf.keras.layers.Embedding(\n",
        "    embedding_matrix.shape[0],\n",
        "    EMBEDDINGS_DIM,\n",
        "    weights=[embedding_matrix],\n",
        "    input_length=MAX_SEQUENCE_LEN,\n",
        "    trainable=False,\n",
        "    name=\"word_embedding\"\n",
        ")(description_input)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adding embedding of size 77 for layer Breed1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsWcm5VLP1g2",
        "colab_type": "text"
      },
      "source": [
        "### Definiendo la red que trabajará con el texto\n",
        "\n",
        "Antes de generar el *feature map* final entre los inputs y las clases, tenemos que generar el *feature map* de las secuencias de texto. Se puede utilizar una red neuronal recurrente o convolucional (recordemos que las redes se utilizan para hacer aprendizaje de representaciones).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIJLjidO6bcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a CNN for the description input\n",
        "FILTER_WIDTHS = [10, 15, 20]  # num of words\n",
        "FILTER_COUNT = 20\n",
        "\n",
        "conv_layers = [] \n",
        "for filter_width in FILTER_WIDTHS: \n",
        "    layer = tf.keras.layers.Conv1D(\n",
        "        FILTER_COUNT, # the number of output filters in the convolution\n",
        "        filter_width, # kernel_size\n",
        "        activation=\"relu\",\n",
        "        name=\"conv_{}_words\".format(filter_width)\n",
        "    )(word_embeddings_layer)\n",
        "    layer = tf.keras.layers.GlobalMaxPooling1D(name=\"max_pool_{}_words\".format(filter_width))(layer)\n",
        "    conv_layers.append(layer)\n",
        "\n",
        "description_features = tf.keras.layers.Concatenate(name=\"convolved_features\")(conv_layers)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux_DAOUjP1hP",
        "colab_type": "text"
      },
      "source": [
        "### Definiendo el *feature map* final de la red\n",
        "\n",
        "Ahora que tenemos nuestra representación de las descripciones, pasamos a combinarlo con los demás features en la última parte de nuestra red."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8xQ3h7VP1hV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_LAYER_SIZE1 = 80\n",
        "HIDDEN_LAYER_SIZE2 = 10\n",
        "\n",
        "feature_map = tf.keras.layers.Concatenate(name=\"feature_map\")(\n",
        "    embedding_layers + [description_features, direct_features_input] + numeric_imputs\n",
        ")\n",
        "\n",
        "dense1 = tf.keras.layers.Dense(HIDDEN_LAYER_SIZE1, activation=\"relu\", kernel_initializer='RandomNormal', \n",
        "                    kernel_regularizer=regularizers.l2(0.001))(feature_map)\n",
        "norm1 = tf.keras.layers.BatchNormalization(momentum=0)(dense1)\n",
        "dropout1 = tf.keras.layers.Dropout(0.5)(norm1)\n",
        "dense2 = tf.keras.layers.Dense(HIDDEN_LAYER_SIZE2, activation=\"relu\", kernel_initializer='RandomNormal', \n",
        "                    kernel_regularizer=regularizers.l2(0.001))(dropout1)\n",
        "norm2 = tf.keras.layers.BatchNormalization(momentum=0)(dense2)\n",
        "dropout2 = tf.keras.layers.Dropout(0.5)(norm2)\n",
        "output_layer = tf.keras.layers.Dense(nlabels, activation=\"softmax\", name=\"output\")(dropout2)\n",
        "\n",
        "model = tf.keras.models.Model(inputs=inputs, outputs=[output_layer], name=\"amazing_model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAGjGthuP1hp",
        "colab_type": "text"
      },
      "source": [
        "### Compilando y visualizando el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MkhQw65P1ht",
        "colab_type": "code",
        "outputId": "ba4d05c1-2c1f-4ae1-9b26-e65c03e65e70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer='nadam',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"amazing_model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "description (InputLayer)        [(None, 55)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "word_embedding (Embedding)      (None, 55, 100)      1000200     description[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Breed1 (InputLayer)             [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_10_words (Conv1D)          (None, 46, 20)       20020       word_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv_15_words (Conv1D)          (None, 41, 20)       30020       word_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv_20_words (Conv1D)          (None, 36, 20)       40020       word_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 1, 77)        23716       Breed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pool_10_words (GlobalMaxPoo (None, 20)           0           conv_10_words[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pool_15_words (GlobalMaxPoo (None, 20)           0           conv_15_words[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pool_20_words (GlobalMaxPoo (None, 20)           0           conv_20_words[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Squeeze (TensorFlow [(None, 77)]         0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "convolved_features (Concatenate (None, 60)           0           max_pool_10_words[0][0]          \n",
            "                                                                 max_pool_15_words[0][0]          \n",
            "                                                                 max_pool_20_words[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "direct_features (InputLayer)    [(None, 28)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Age_norm (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Fee_norm (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "feature_map (Concatenate)       (None, 167)          0           tf_op_layer_Squeeze[0][0]        \n",
            "                                                                 convolved_features[0][0]         \n",
            "                                                                 direct_features[0][0]            \n",
            "                                                                 Age_norm[0][0]                   \n",
            "                                                                 Fee_norm[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 80)           13440       feature_map[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 80)           320         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 80)           0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           810         dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 10)           40          dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 10)           0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 5)            55          dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,128,641\n",
            "Trainable params: 128,261\n",
            "Non-trainable params: 1,000,380\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqc8PSZoP1ig",
        "colab_type": "code",
        "outputId": "201be5b4-06a0-4a7a-d31c-5df2ea189665",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "SVG(tf.keras.utils.model_to_dot(model, dpi=60).create(prog='dot', format='svg'))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"767pt\" viewBox=\"0.00 0.00 1555.50 921.00\" width=\"1296pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(.8333 .8333) rotate(0) translate(4 917)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-917 1551.5,-917 1551.5,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140199545979120 -->\n<g class=\"node\" id=\"node1\">\n<title>140199545979120</title>\n<polygon fill=\"none\" points=\"358,-876.5 358,-912.5 509,-912.5 509,-876.5 358,-876.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"433.5\" y=\"-890.8\">description: InputLayer</text>\n</g>\n<!-- 140199545611656 -->\n<g class=\"node\" id=\"node2\">\n<title>140199545611656</title>\n<polygon fill=\"none\" points=\"336.5,-803.5 336.5,-839.5 530.5,-839.5 530.5,-803.5 336.5,-803.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"433.5\" y=\"-817.8\">word_embedding: Embedding</text>\n</g>\n<!-- 140199545979120&#45;&gt;140199545611656 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140199545979120-&gt;140199545611656</title>\n<path d=\"M433.5,-876.4551C433.5,-868.3828 433.5,-858.6764 433.5,-849.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"437.0001,-849.5903 433.5,-839.5904 430.0001,-849.5904 437.0001,-849.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140199547853848 -->\n<g class=\"node\" id=\"node4\">\n<title>140199547853848</title>\n<polygon fill=\"none\" points=\"109.5,-730.5 109.5,-766.5 277.5,-766.5 277.5,-730.5 109.5,-730.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"193.5\" y=\"-744.8\">conv_10_words: Conv1D</text>\n</g>\n<!-- 140199545611656&#45;&gt;140199547853848 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140199545611656-&gt;140199547853848</title>\n<path d=\"M374.1741,-803.4551C340.4237,-793.1893 297.9837,-780.2804 262.7183,-769.5539\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"263.5609,-766.1519 252.9752,-766.5904 261.5238,-772.849 263.5609,-766.1519\" stroke=\"#000000\"/>\n</g>\n<!-- 140199545666248 -->\n<g class=\"node\" id=\"node5\">\n<title>140199545666248</title>\n<polygon fill=\"none\" points=\"349.5,-730.5 349.5,-766.5 517.5,-766.5 517.5,-730.5 349.5,-730.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"433.5\" y=\"-744.8\">conv_15_words: Conv1D</text>\n</g>\n<!-- 140199545611656&#45;&gt;140199545666248 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140199545611656-&gt;140199545666248</title>\n<path d=\"M433.5,-803.4551C433.5,-795.3828 433.5,-785.6764 433.5,-776.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"437.0001,-776.5903 433.5,-766.5904 430.0001,-776.5904 437.0001,-776.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140200889921488 -->\n<g class=\"node\" id=\"node6\">\n<title>140200889921488</title>\n<polygon fill=\"none\" points=\"590.5,-730.5 590.5,-766.5 758.5,-766.5 758.5,-730.5 590.5,-730.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"674.5\" y=\"-744.8\">conv_20_words: Conv1D</text>\n</g>\n<!-- 140199545611656&#45;&gt;140200889921488 -->\n<g class=\"edge\" id=\"edge4\">\n<title>140199545611656-&gt;140200889921488</title>\n<path d=\"M493.073,-803.4551C526.9641,-793.1893 569.581,-780.2804 604.9932,-769.5539\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"606.2211,-772.8391 614.777,-766.5904 604.1918,-766.1397 606.2211,-772.8391\" stroke=\"#000000\"/>\n</g>\n<!-- 140199785442496 -->\n<g class=\"node\" id=\"node3\">\n<title>140199785442496</title>\n<polygon fill=\"none\" points=\"899,-730.5 899,-766.5 1028,-766.5 1028,-730.5 899,-730.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"963.5\" y=\"-744.8\">Breed1: InputLayer</text>\n</g>\n<!-- 140199694481168 -->\n<g class=\"node\" id=\"node7\">\n<title>140199694481168</title>\n<polygon fill=\"none\" points=\"885.5,-657.5 885.5,-693.5 1041.5,-693.5 1041.5,-657.5 885.5,-657.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"963.5\" y=\"-671.8\">embedding: Embedding</text>\n</g>\n<!-- 140199785442496&#45;&gt;140199694481168 -->\n<g class=\"edge\" id=\"edge5\">\n<title>140199785442496-&gt;140199694481168</title>\n<path d=\"M963.5,-730.4551C963.5,-722.3828 963.5,-712.6764 963.5,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"967.0001,-703.5903 963.5,-693.5904 960.0001,-703.5904 967.0001,-703.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140199662113904 -->\n<g class=\"node\" id=\"node8\">\n<title>140199662113904</title>\n<polygon fill=\"none\" points=\"0,-657.5 0,-693.5 277,-693.5 277,-657.5 0,-657.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"138.5\" y=\"-671.8\">max_pool_10_words: GlobalMaxPooling1D</text>\n</g>\n<!-- 140199547853848&#45;&gt;140199662113904 -->\n<g class=\"edge\" id=\"edge6\">\n<title>140199547853848-&gt;140199662113904</title>\n<path d=\"M179.9045,-730.4551C173.3599,-721.7686 165.3896,-711.1898 158.196,-701.642\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"160.9427,-699.4711 152.1297,-693.5904 155.3518,-703.6834 160.9427,-699.4711\" stroke=\"#000000\"/>\n</g>\n<!-- 140199663029776 -->\n<g class=\"node\" id=\"node9\">\n<title>140199663029776</title>\n<polygon fill=\"none\" points=\"295,-657.5 295,-693.5 572,-693.5 572,-657.5 295,-657.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"433.5\" y=\"-671.8\">max_pool_15_words: GlobalMaxPooling1D</text>\n</g>\n<!-- 140199545666248&#45;&gt;140199663029776 -->\n<g class=\"edge\" id=\"edge7\">\n<title>140199545666248-&gt;140199663029776</title>\n<path d=\"M433.5,-730.4551C433.5,-722.3828 433.5,-712.6764 433.5,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"437.0001,-703.5903 433.5,-693.5904 430.0001,-703.5904 437.0001,-703.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140199545420264 -->\n<g class=\"node\" id=\"node10\">\n<title>140199545420264</title>\n<polygon fill=\"none\" points=\"590,-657.5 590,-693.5 867,-693.5 867,-657.5 590,-657.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"728.5\" y=\"-671.8\">max_pool_20_words: GlobalMaxPooling1D</text>\n</g>\n<!-- 140200889921488&#45;&gt;140199545420264 -->\n<g class=\"edge\" id=\"edge8\">\n<title>140200889921488-&gt;140199545420264</title>\n<path d=\"M687.8483,-730.4551C694.2739,-721.7686 702.0993,-711.1898 709.1621,-701.642\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"711.9849,-703.7113 715.1181,-693.5904 706.3572,-699.5484 711.9849,-703.7113\" stroke=\"#000000\"/>\n</g>\n<!-- 140199545153912 -->\n<g class=\"node\" id=\"node11\">\n<title>140199545153912</title>\n<polygon fill=\"none\" points=\"762,-584.5 762,-620.5 1027,-620.5 1027,-584.5 762,-584.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"894.5\" y=\"-598.8\">tf_op_layer_Squeeze: TensorFlowOpLayer</text>\n</g>\n<!-- 140199694481168&#45;&gt;140199545153912 -->\n<g class=\"edge\" id=\"edge9\">\n<title>140199694481168-&gt;140199545153912</title>\n<path d=\"M946.4438,-657.4551C938.0675,-648.5932 927.8295,-637.7616 918.6638,-628.0646\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"921.0119,-625.4535 911.5991,-620.5904 915.9247,-630.262 921.0119,-625.4535\" stroke=\"#000000\"/>\n</g>\n<!-- 140199547853624 -->\n<g class=\"node\" id=\"node12\">\n<title>140199547853624</title>\n<polygon fill=\"none\" points=\"468.5,-584.5 468.5,-620.5 674.5,-620.5 674.5,-584.5 468.5,-584.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"571.5\" y=\"-598.8\">convolved_features: Concatenate</text>\n</g>\n<!-- 140199662113904&#45;&gt;140199547853624 -->\n<g class=\"edge\" id=\"edge10\">\n<title>140199662113904-&gt;140199547853624</title>\n<path d=\"M245.5337,-657.4551C310.2074,-646.5516 392.5666,-632.6666 458.3179,-621.5815\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"459.029,-625.0111 468.3079,-619.8973 457.8652,-618.1085 459.029,-625.0111\" stroke=\"#000000\"/>\n</g>\n<!-- 140199663029776&#45;&gt;140199547853624 -->\n<g class=\"edge\" id=\"edge11\">\n<title>140199663029776-&gt;140199547853624</title>\n<path d=\"M467.6124,-657.4551C485.9407,-647.7596 508.7259,-635.7066 528.2832,-625.361\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"530.0989,-628.3602 537.3018,-620.5904 526.8257,-622.1725 530.0989,-628.3602\" stroke=\"#000000\"/>\n</g>\n<!-- 140199545420264&#45;&gt;140199547853624 -->\n<g class=\"edge\" id=\"edge12\">\n<title>140199545420264-&gt;140199547853624</title>\n<path d=\"M689.691,-657.4551C668.5561,-647.628 642.2122,-635.3789 619.7629,-624.9407\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"620.9501,-621.6329 610.4067,-620.5904 617.9987,-627.9803 620.9501,-621.6329\" stroke=\"#000000\"/>\n</g>\n<!-- 140199545153408 -->\n<g class=\"node\" id=\"node16\">\n<title>140199545153408</title>\n<polygon fill=\"none\" points=\"1048.5,-511.5 1048.5,-547.5 1212.5,-547.5 1212.5,-511.5 1048.5,-511.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1130.5\" y=\"-525.8\">feature_map: Concatenate</text>\n</g>\n<!-- 140199545153912&#45;&gt;140199545153408 -->\n<g class=\"edge\" id=\"edge13\">\n<title>140199545153912-&gt;140199545153408</title>\n<path d=\"M952.8371,-584.4551C986.025,-574.1893 1027.7577,-561.2804 1062.4353,-550.5539\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1063.497,-553.8892 1072.0161,-547.5904 1061.4284,-547.2018 1063.497,-553.8892\" stroke=\"#000000\"/>\n</g>\n<!-- 140199547853624&#45;&gt;140199545153408 -->\n<g class=\"edge\" id=\"edge14\">\n<title>140199547853624-&gt;140199545153408</title>\n<path d=\"M674.6807,-589.0256C778.8361,-575.4239 938.2048,-554.6119 1038.2681,-541.5446\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1038.9006,-544.9918 1048.3631,-540.2263 1037.9941,-538.0507 1038.9006,-544.9918\" stroke=\"#000000\"/>\n</g>\n<!-- 140199663033872 -->\n<g class=\"node\" id=\"node13\">\n<title>140199663033872</title>\n<polygon fill=\"none\" points=\"1045.5,-584.5 1045.5,-620.5 1215.5,-620.5 1215.5,-584.5 1045.5,-584.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1130.5\" y=\"-598.8\">direct_features: InputLayer</text>\n</g>\n<!-- 140199663033872&#45;&gt;140199545153408 -->\n<g class=\"edge\" id=\"edge15\">\n<title>140199663033872-&gt;140199545153408</title>\n<path d=\"M1130.5,-584.4551C1130.5,-576.3828 1130.5,-566.6764 1130.5,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1134.0001,-557.5903 1130.5,-547.5904 1127.0001,-557.5904 1134.0001,-557.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140199545979064 -->\n<g class=\"node\" id=\"node14\">\n<title>140199545979064</title>\n<polygon fill=\"none\" points=\"1233.5,-584.5 1233.5,-620.5 1383.5,-620.5 1383.5,-584.5 1233.5,-584.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1308.5\" y=\"-598.8\">Age_norm: InputLayer</text>\n</g>\n<!-- 140199545979064&#45;&gt;140199545153408 -->\n<g class=\"edge\" id=\"edge16\">\n<title>140199545979064-&gt;140199545153408</title>\n<path d=\"M1264.5,-584.4551C1240.2172,-574.4964 1209.8695,-562.0504 1184.1982,-551.5223\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1185.191,-548.1466 1174.6107,-547.5904 1182.5348,-554.6231 1185.191,-548.1466\" stroke=\"#000000\"/>\n</g>\n<!-- 140199545982760 -->\n<g class=\"node\" id=\"node15\">\n<title>140199545982760</title>\n<polygon fill=\"none\" points=\"1401.5,-584.5 1401.5,-620.5 1547.5,-620.5 1547.5,-584.5 1401.5,-584.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1474.5\" y=\"-598.8\">Fee_norm: InputLayer</text>\n</g>\n<!-- 140199545982760&#45;&gt;140199545153408 -->\n<g class=\"edge\" id=\"edge17\">\n<title>140199545982760-&gt;140199545153408</title>\n<path d=\"M1401.1959,-585.8978C1398.2598,-585.2531 1395.3525,-584.6184 1392.5,-584 1336.0985,-571.7723 1272.745,-558.5792 1222.4658,-548.2366\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1223.0901,-544.7918 1212.5902,-546.2072 1221.681,-551.6485 1223.0901,-544.7918\" stroke=\"#000000\"/>\n</g>\n<!-- 140199545154920 -->\n<g class=\"node\" id=\"node17\">\n<title>140199545154920</title>\n<polygon fill=\"none\" points=\"1084.5,-438.5 1084.5,-474.5 1176.5,-474.5 1176.5,-438.5 1084.5,-438.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1130.5\" y=\"-452.8\">dense: Dense</text>\n</g>\n<!-- 140199545153408&#45;&gt;140199545154920 -->\n<g class=\"edge\" id=\"edge18\">\n<title>140199545153408-&gt;140199545154920</title>\n<path d=\"M1130.5,-511.4551C1130.5,-503.3828 1130.5,-493.6764 1130.5,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1134.0001,-484.5903 1130.5,-474.5904 1127.0001,-484.5904 1134.0001,-484.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140199545153688 -->\n<g class=\"node\" id=\"node18\">\n<title>140199545153688</title>\n<polygon fill=\"none\" points=\"1002.5,-365.5 1002.5,-401.5 1258.5,-401.5 1258.5,-365.5 1002.5,-365.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1130.5\" y=\"-379.8\">batch_normalization: BatchNormalization</text>\n</g>\n<!-- 140199545154920&#45;&gt;140199545153688 -->\n<g class=\"edge\" id=\"edge19\">\n<title>140199545154920-&gt;140199545153688</title>\n<path d=\"M1130.5,-438.4551C1130.5,-430.3828 1130.5,-420.6764 1130.5,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1134.0001,-411.5903 1130.5,-401.5904 1127.0001,-411.5904 1134.0001,-411.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140199662121424 -->\n<g class=\"node\" id=\"node19\">\n<title>140199662121424</title>\n<polygon fill=\"none\" points=\"1071,-292.5 1071,-328.5 1190,-328.5 1190,-292.5 1071,-292.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1130.5\" y=\"-306.8\">dropout: Dropout</text>\n</g>\n<!-- 140199545153688&#45;&gt;140199662121424 -->\n<g class=\"edge\" id=\"edge20\">\n<title>140199545153688-&gt;140199662121424</title>\n<path d=\"M1130.5,-365.4551C1130.5,-357.3828 1130.5,-347.6764 1130.5,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1134.0001,-338.5903 1130.5,-328.5904 1127.0001,-338.5904 1134.0001,-338.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140199544746616 -->\n<g class=\"node\" id=\"node20\">\n<title>140199544746616</title>\n<polygon fill=\"none\" points=\"1077,-219.5 1077,-255.5 1184,-255.5 1184,-219.5 1077,-219.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1130.5\" y=\"-233.8\">dense_1: Dense</text>\n</g>\n<!-- 140199662121424&#45;&gt;140199544746616 -->\n<g class=\"edge\" id=\"edge21\">\n<title>140199662121424-&gt;140199544746616</title>\n<path d=\"M1130.5,-292.4551C1130.5,-284.3828 1130.5,-274.6764 1130.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1134.0001,-265.5903 1130.5,-255.5904 1127.0001,-265.5904 1134.0001,-265.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140199544746504 -->\n<g class=\"node\" id=\"node21\">\n<title>140199544746504</title>\n<polygon fill=\"none\" points=\"995,-146.5 995,-182.5 1266,-182.5 1266,-146.5 995,-146.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1130.5\" y=\"-160.8\">batch_normalization_1: BatchNormalization</text>\n</g>\n<!-- 140199544746616&#45;&gt;140199544746504 -->\n<g class=\"edge\" id=\"edge22\">\n<title>140199544746616-&gt;140199544746504</title>\n<path d=\"M1130.5,-219.4551C1130.5,-211.3828 1130.5,-201.6764 1130.5,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1134.0001,-192.5903 1130.5,-182.5904 1127.0001,-192.5904 1134.0001,-192.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140199544327640 -->\n<g class=\"node\" id=\"node22\">\n<title>140199544327640</title>\n<polygon fill=\"none\" points=\"1063.5,-73.5 1063.5,-109.5 1197.5,-109.5 1197.5,-73.5 1063.5,-73.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1130.5\" y=\"-87.8\">dropout_1: Dropout</text>\n</g>\n<!-- 140199544746504&#45;&gt;140199544327640 -->\n<g class=\"edge\" id=\"edge23\">\n<title>140199544746504-&gt;140199544327640</title>\n<path d=\"M1130.5,-146.4551C1130.5,-138.3828 1130.5,-128.6764 1130.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1134.0001,-119.5903 1130.5,-109.5904 1127.0001,-119.5904 1134.0001,-119.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140199544421792 -->\n<g class=\"node\" id=\"node23\">\n<title>140199544421792</title>\n<polygon fill=\"none\" points=\"1082,-.5 1082,-36.5 1179,-36.5 1179,-.5 1082,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1130.5\" y=\"-14.8\">output: Dense</text>\n</g>\n<!-- 140199544327640&#45;&gt;140199544421792 -->\n<g class=\"edge\" id=\"edge24\">\n<title>140199544327640-&gt;140199544421792</title>\n<path d=\"M1130.5,-73.4551C1130.5,-65.3828 1130.5,-55.6764 1130.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1134.0001,-46.5903 1130.5,-36.5904 1127.0001,-46.5904 1134.0001,-46.5903\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbwjlkxbP1i6",
        "colab_type": "text"
      },
      "source": [
        "## Entrenando el modelo\n",
        "\n",
        "Para entrenar el modelo es igual al caso anterior, ya generados el conjunto de datos correspondiente. Lo entrenamos con ayuda de `mlflow`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2AUAUCBP1jB",
        "colab_type": "code",
        "outputId": "6ea72738-8075-49a9-8cd5-5453432a819e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "import mlflow\n",
        "\n",
        "mlflow.set_experiment('awesome_advanced_approach')\n",
        "\n",
        "with mlflow.start_run(nested=True):\n",
        "    # Log model hiperparameters first\n",
        "    mlflow.log_param('description_filter_widths', FILTER_WIDTHS)\n",
        "    mlflow.log_param('description_filter_count', FILTER_COUNT)\n",
        "    mlflow.log_param('hidden_layer_size1', HIDDEN_LAYER_SIZE1)\n",
        "    mlflow.log_param('hidden_layer_size2', HIDDEN_LAYER_SIZE2)\n",
        "    mlflow.log_param('embedded_columns', embedded_columns)\n",
        "    mlflow.log_param('one_hot_columns', one_hot_columns)\n",
        "    mlflow.log_param('numerical_columns', numeric_columns) \n",
        "    \n",
        "    # Train\n",
        "    epochs = 10\n",
        "    history = model.fit(train_dataset, epochs=epochs)\n",
        "    \n",
        "    # Evaluate\n",
        "    loss, accuracy = model.evaluate(dev_dataset, verbose=0)\n",
        "    print(\"\\n*** Validation loss: {} - accuracy: {}\".format(loss, accuracy))\n",
        "    mlflow.log_metric('epochs', epochs)\n",
        "    mlflow.log_metric('train_loss', history.history[\"loss\"][-1])\n",
        "    mlflow.log_metric('train_accuracy', history.history[\"accuracy\"][-1])\n",
        "    mlflow.log_metric('validation_loss', loss)\n",
        "    mlflow.log_metric('validation_accuracy', accuracy)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "67/67 [==============================] - 23s 347ms/step - loss: 2.1445 - accuracy: 0.2348\n",
            "Epoch 2/10\n",
            "67/67 [==============================] - 20s 296ms/step - loss: 1.8619 - accuracy: 0.2718\n",
            "Epoch 3/10\n",
            "67/67 [==============================] - 20s 296ms/step - loss: 1.7201 - accuracy: 0.2937\n",
            "Epoch 4/10\n",
            "67/67 [==============================] - 20s 293ms/step - loss: 1.6394 - accuracy: 0.3234\n",
            "Epoch 5/10\n",
            "67/67 [==============================] - 19s 287ms/step - loss: 1.5748 - accuracy: 0.3374\n",
            "Epoch 6/10\n",
            "67/67 [==============================] - 19s 286ms/step - loss: 1.5318 - accuracy: 0.3610\n",
            "Epoch 7/10\n",
            "67/67 [==============================] - 20s 296ms/step - loss: 1.4884 - accuracy: 0.3841\n",
            "Epoch 8/10\n",
            "67/67 [==============================] - 20s 301ms/step - loss: 1.4415 - accuracy: 0.4005\n",
            "Epoch 9/10\n",
            "67/67 [==============================] - 20s 292ms/step - loss: 1.4037 - accuracy: 0.4215\n",
            "Epoch 10/10\n",
            "67/67 [==============================] - 19s 287ms/step - loss: 1.3594 - accuracy: 0.4337\n",
            "\n",
            "*** Validation loss: 1.5632886255488676 - accuracy: 0.36513933539390564\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvFk3WYa3tV3",
        "colab_type": "text"
      },
      "source": [
        "# CONCLUSIONES:\n",
        "\n",
        "Se probaron varios hiperparámetros de la red para ver como afectaban al resultado final:\n",
        "\n",
        "Los features map que se obtienen luego de concatenar todas las representaciones de la entrada son de 167 valores.\n",
        "Se probaron dos capas densas, y para paliar el overfitting se usó el regularizador l2 y una capa dropout()\n",
        "Además se usaron capas BatchNormalization.\n",
        "\n",
        "Se observó que al usar una mayor probabilidad de dropout la red tarda más en aprender el conjunto de entrenamiento entre una época y la siguiente.\n",
        "\n",
        "Filtros usados en la red cnn para crear la representación de las description de las mascotas.\n",
        "* anchos de los filtros: se probó con  [5, 10, 15] y [10, 15, 20] optando por el segundo. Se evitó usar anchos muy pequeños en relación a la longitud de la sequencia en un intento por captar información más general de la description.\n",
        "* cantidad de filtros por cada ancho: 20\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UcxUXY14hsk",
        "colab_type": "text"
      },
      "source": [
        "=================================================================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUFsh1nMP1jP",
        "colab_type": "text"
      },
      "source": [
        "## Evaluando el modelo sobre los datos de evaluación para la competencia\n",
        "\n",
        "Una vez que tenemos definido nuestro modelo, el último paso es ponerlo a prueba en los datos de evaluación para generar un archivo para enviar a la competencia Kaggle.\n",
        "\n",
        "Comenzamos cargando el conjunto de datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbcWEifyP1jS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "a090cfb8-fa96-4838-f9b0-94c5c6c84454"
      },
      "source": [
        "test_dataset = pd.read_csv(os.path.join(DATA_DIRECTORY, 'test.csv'))\n",
        "test_dataset.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Type</th>\n",
              "      <th>Age</th>\n",
              "      <th>Breed1</th>\n",
              "      <th>Breed2</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Color1</th>\n",
              "      <th>Color2</th>\n",
              "      <th>Color3</th>\n",
              "      <th>MaturitySize</th>\n",
              "      <th>FurLength</th>\n",
              "      <th>Vaccinated</th>\n",
              "      <th>Dewormed</th>\n",
              "      <th>Sterilized</th>\n",
              "      <th>Health</th>\n",
              "      <th>Quantity</th>\n",
              "      <th>Fee</th>\n",
              "      <th>State</th>\n",
              "      <th>Description</th>\n",
              "      <th>PID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>265</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>41401</td>\n",
              "      <td>I just found it alone yesterday near my apartm...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>307</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>41326</td>\n",
              "      <td>Their pregnant mother was dumped by her irresp...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>307</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>41326</td>\n",
              "      <td>Siu Pak just give birth on 13/6/10 to 6puppies...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>265</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>41326</td>\n",
              "      <td>Very manja and gentle stray cat found, we woul...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>264</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>41326</td>\n",
              "      <td>Kali is a super playful kitten who is on the g...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Type  Age  ...                                        Description  PID\n",
              "0     2    1  ...  I just found it alone yesterday near my apartm...    1\n",
              "1     1    1  ...  Their pregnant mother was dumped by her irresp...    2\n",
              "2     1    0  ...  Siu Pak just give birth on 13/6/10 to 6puppies...    7\n",
              "3     2   12  ...  Very manja and gentle stray cat found, we woul...    9\n",
              "4     2    3  ...  Kali is a super playful kitten who is on the g...   11\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtFGOjl0P1jl",
        "colab_type": "text"
      },
      "source": [
        "## Creamos el conjunto de datos para darle al modelo entrenado\n",
        "\n",
        "Tenemos que preprocesar los datos de evaluación de la misma manera que preprocesamos los de entrenamiento (para que sean compatibles con lo esperado por el modelo). Por suerte, es tan simple como hacer un par de modificaciones a lo ya hecho previamente. Lo único que tenemos que tener en cuenta es que ahora el conjunto de datos no generará una etiqueta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNWvjo5bP1jr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "2ef91403-eaa6-45a9-d358-0ab0918a22e9"
      },
      "source": [
        "# First tokenize the description\n",
        "\n",
        "test_dataset[\"TokenizedDescription\"] = test_dataset[\"Description\"]\\\n",
        "    .fillna(value=\"\").apply(tokenize_description)\n",
        "\n",
        "# Normalización de features numéricos\n",
        "scaler = StandardScaler()\n",
        "numeric_columns = ['Age', 'Fee']\n",
        "norm = scaler.fit_transform(test_dataset[numeric_columns])\n",
        "dfnorm = pd.DataFrame({'Age_norm':norm[:,0] ,'Fee_norm':norm[:,1]})\n",
        "test_dataset = pd.concat([dfnorm, test_dataset], axis=1)\n",
        "numeric_columns = ['Age_norm', 'Fee_norm']\n",
        "\n",
        "# Generate the basic TF dataset\n",
        "\n",
        "tf_test_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: dataset_generator(test_dataset, True),\n",
        "    output_types=instance_types  # It should have the same instance types\n",
        ")\n",
        "\n",
        "for data in tf_test_dataset.take(2):  # The dataset only returns a data instance now (no target)\n",
        "    pprint(data)\n",
        "    print()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Age_norm': <tf.Tensor: id=5982, shape=(1,), dtype=float32, numpy=array([-0.52713054], dtype=float32)>,\n",
            " 'Breed1': <tf.Tensor: id=5983, shape=(1,), dtype=int32, numpy=array([265], dtype=int32)>,\n",
            " 'Fee_norm': <tf.Tensor: id=5984, shape=(1,), dtype=float32, numpy=array([-0.28477862], dtype=float32)>,\n",
            " 'description': <tf.Tensor: id=5985, shape=(13,), dtype=int32, numpy=\n",
            "array([ 116,  429, 1371,  991,  189,    1, 7873, 1043,   62,  600,  728,\n",
            "          5,    1], dtype=int32)>,\n",
            " 'direct_features': <tf.Tensor: id=5986, shape=(28,), dtype=float32, numpy=\n",
            "array([0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
            "       0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.], dtype=float32)>}\n",
            "\n",
            "{'Age_norm': <tf.Tensor: id=5987, shape=(1,), dtype=float32, numpy=array([-0.52713054], dtype=float32)>,\n",
            " 'Breed1': <tf.Tensor: id=5988, shape=(1,), dtype=int32, numpy=array([307], dtype=int32)>,\n",
            " 'Fee_norm': <tf.Tensor: id=5989, shape=(1,), dtype=float32, numpy=array([-0.28477862], dtype=float32)>,\n",
            " 'description': <tf.Tensor: id=5990, shape=(47,), dtype=int32, numpy=\n",
            "array([ 945,  154,  256, 2049,  105,  403,  991, 4677,  552,  545,    1,\n",
            "        142,  134,  403,    1,  118,  210,   73,    1,  533,  387,   35,\n",
            "        394,  272,   98,   62,    1,  464,  411,  151, 1401,   42,  253,\n",
            "          1,  825,   35, 4659,  247, 4155, 1402, 1403,    1,   43,   52,\n",
            "        599,   38,    1], dtype=int32)>,\n",
            " 'direct_features': <tf.Tensor: id=5991, shape=(28,), dtype=float32, numpy=\n",
            "array([1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
            "       0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.], dtype=float32)>}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzckO7FWP1j5",
        "colab_type": "text"
      },
      "source": [
        "## Padding batches\n",
        "\n",
        "Por último, y previo a probar el modelo sobre los datos de evaluación, generamos el conjunto de datos \"rellenado\". \n",
        "\n",
        "A diferencia de los datos de entrenamiento y validación, en este caso no permutamos las instancias, pues necesitamos saber a que `PID` pertenece cada una.\n",
        "\n",
        "Por otra parte, utilizamos los mismos valores de `padding_shapes` y `padding_values` para el primer componente (el de los datos), ignorando el valor del segundo componente (el de las etiquetas)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YicLC5aVP1j9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = tf_test_dataset.padded_batch(\n",
        "    BATCH_SIZE, \n",
        "    padded_shapes=padding_shapes[0], \n",
        "    padding_values=padding_values[0]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZC8VKsPP1kO",
        "colab_type": "text"
      },
      "source": [
        "## Correr el modelo\n",
        "\n",
        "El último paso es correr el modelo sobre los datos de evaluación para conseguir las predicciones a enviar a la competencia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDNC7VcEP1kR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset[\"AdoptionSpeed\"] = model.predict(test_data).argmax(axis=1)\n",
        "\n",
        "test_dataset.to_csv(\"./submission.csv\", index=False, columns=[\"PID\", \"AdoptionSpeed\"])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}